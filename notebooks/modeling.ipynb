{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7984d63-91d5-448b-b034-b571fdc5f6b9",
   "metadata": {},
   "source": [
    "# Моделирование"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b511b963-b2c5-4d80-bee3-70150d54d01e",
   "metadata": {},
   "source": [
    "Импортируем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "456c7dde-880e-41f8-8636-f250ea6f69dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tqdm as notebook_tqdm\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "6fd47c2a-500e-431f-9e62-1a39a6e78702",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191385b2-d801-4a9d-b395-d7479ba10fca",
   "metadata": {},
   "source": [
    "## Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "5557fee8-2202-4dae-b5a5-689db75b16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "SOURCE_PATH = os.path.join(DATA_DIR, \"cleaned_data.csv\")\n",
    "\n",
    "data = pd.read_csv(SOURCE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "04ac3c5c-c6f5-4142-89b6-4fbbee476f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>awww thats a bummer you shoulda got david carr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>no its not behaving at all im mad why am i her...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                 date             user  \\\n",
       "0       0  1467810369  2009-04-06 22:19:45  _TheSpecialOne_   \n",
       "1       0  1467810672  2009-04-06 22:19:49    scotthamilton   \n",
       "2       0  1467810917  2009-04-06 22:19:53         mattycus   \n",
       "3       0  1467811184  2009-04-06 22:19:57          ElleCTF   \n",
       "4       0  1467811193  2009-04-06 22:19:57           Karoli   \n",
       "\n",
       "                                                text  \n",
       "0  awww thats a bummer you shoulda got david carr...  \n",
       "1  is upset that he cant update his facebook by t...  \n",
       "2  i dived many times for the ball managed to sav...  \n",
       "3     my whole body feels itchy and like its on fire  \n",
       "4  no its not behaving at all im mad why am i her...  "
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "42116548-6783-448a-bfb7-09b9deea53cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Обучить на всей выборке\n",
    "data = data.sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "21c3dcb6-34be-4f59-9136-123c73b4d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиваем данные на тренировочную и тестовую выборки\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split( \\\n",
    "    data['text'].tolist(), data['target'].tolist(), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "70151488-0d7c-4d6b-8fdb-7f9982e0f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенизируем данные\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "25569213-0b2a-440e-a28e-46fed34bf0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем вспомогательный класс для BERT-модели\n",
    "class Sentiment140Dataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "6c717bf1-39e7-4c51-b530-a8592a41494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Sentiment140Dataset(train_encodings, train_labels)\n",
    "val_dataset = Sentiment140Dataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "cbb2e9c0-adb1-4e28-8925-89ba83a9c991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Загружаем модель\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased', output_attentions=True, num_labels=2, attn_implementation=\"eager\"`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "542eeb1a-d4d3-4def-a2fd-ade3569919ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем тренировочный пайплайн\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "107d95dc-49d1-469b-90e5-d680b521f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "bb5df7e6-3a71-4414-b7fd-15a55fc05bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DistilBertSdpaAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "Epoch 1/3: 100%|███████████████████████████████████████████████████████████████████████████████| 760/760 [10:16<00:00,  1.23it/s, loss=0.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|███████████████████████████████████████████████████████████████████████████████| 760/760 [13:52<00:00,  1.09s/it, loss=0.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████████████████████████████████████████████████████████████████████████| 760/760 [12:12<00:00,  1.04it/s, loss=0.259]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    total_loss = 0\n",
    "    num_batches = len(train_loader)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1} completed - Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "eca585fb-179f-4ab3-a24c-f134446f7d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оцениваем модель\n",
    "model.eval()\n",
    "preds, true_labels = [], []\n",
    "\n",
    "for batch in val_loader:\n",
    "    inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "    true_labels.extend(inputs['labels'].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "4f89b69e-50a3-4fdd-b889-7d17e1de3bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7753\n"
     ]
    }
   ],
   "source": [
    "# Расчитываем Accuracy\n",
    "accuracy = accuracy_score(true_labels, preds)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a828a5d7-67f8-45fc-9557-9a3eb0c93b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/tokenizer_config.json',\n",
       " '../models/special_tokens_map.json',\n",
       " '../models/vocab.txt',\n",
       " '../models/added_tokens.json')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сохраняем модель и токенизатор\n",
    "MODEL_DIR = \"../models\"\n",
    "\n",
    "model.save_pretrained(MODEL_DIR)\n",
    "tokenizer.save_pretrained(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3546aa2e-e32d-439f-9a32-ed4391ba3b53",
   "metadata": {},
   "source": [
    "## Использование модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "7431562f-82b3-4040-8bab-fd0e5d9b810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sentiment Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = \"../models\"\n",
    "\n",
    "# Загружаем модель\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "\n",
    "# Загружаем токенизатор\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"✅ Sentiment Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "99cea98a-f1b5-470c-8f95-a348ce166187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказываем тональнось текста\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    # Токенизируем входной текст\n",
    "    inputs = tokenizer(text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    # Выполняем предсказание\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "    attention_weights = outputs.attentions  # Получаем веса внимания\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    \n",
    "    # Получаем предсказание\n",
    "    predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "    negative_prob, positive_prob = probs[0].tolist()  # Достаем вероятности\n",
    "\n",
    "    # Определяем класс с учетом нейтрального диапазона\n",
    "    # Такой диапазон подобран на глазок, так как в данных не содержатся нейтральные примеры\n",
    "    if 0.1 <= positive_prob <= 0.9:\n",
    "        sentiment_label = \"Neutral\"\n",
    "    else:\n",
    "        sentiment_label = \"Positive\" if positive_prob > 0.5 else \"Negative\"\n",
    "\n",
    "    # Получаем тензоры внимания для каждого слоя модели\n",
    "    attentions = outputs.attentions\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "    # Аггрегируем значения внимания\n",
    "    word_importance = {token: 0.0 for token in tokens}\n",
    "\n",
    "    num_layers = len(attentions)\n",
    "    num_heads = attentions[0].shape[1]\n",
    "\n",
    "    for layer in range(num_layers):\n",
    "        attn_layer = attentions[layer][0]  # Форма тензора: (num_heads, seq_len, seq_len)\n",
    "        avg_attn = attn_layer.mean(dim=0) # Усреднение по всем вершинам -> (seq_len, seq_len)\n",
    "\n",
    "        # Суммируем значения внимания, полученные каждым токеном (исключая диагональ)\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token in [\"[CLS]\", \"[SEP]\"] or token in string.punctuation or token.lower() in stop_words:\n",
    "                continue\n",
    "            word_importance[token] += avg_attn[:, i].sum().item()  # Sum of incoming attention\n",
    "\n",
    "    # Нормализация значений важности\n",
    "    max_importance = max(word_importance.values()) if word_importance else 1.0\n",
    "    word_importance = {token: round(score / max_importance, 2) for token, score in word_importance.items()}\n",
    "\n",
    "    # Сортируем по важности\n",
    "    sorted_importance = sorted(word_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    # Отфильтровываем ненужные слова\n",
    "    filtered_importance = [\n",
    "        x for x in sorted_importance\n",
    "        if x[0] not in {\"[CLS]\", \"[SEP]\"} \n",
    "        and x[0] not in string.punctuation \n",
    "        and x[0].lower() not in stop_words\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"sentiment\": sentiment_label,\n",
    "        \"importance\": filtered_importance\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "78989d27-fc11-44f7-b4b9-4e6a5d5bb359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentiment': 'Positive', 'importance': [('movie', 1.0), ('loved', 0.84), ('every', 0.56), ('amazing', 0.53), ('part', 0.49), ('absolutely', 0.43)]}\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\"The movie was absolutely amazing, I loved every part of it!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "325fa1aa-8db8-428e-93dc-94a5955c17cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentiment': 'Negative', 'importance': [('hate', 1.0), ('product', 0.76)]}\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\"I hate the product!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "6744d33f-2aab-48cf-9fd2-6c190f288198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentiment': 'Neutral', 'importance': [('blue', 1.0), ('sky', 0.62)]}\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\"Sky is blue\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
